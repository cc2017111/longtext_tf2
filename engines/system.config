### use # to comment out the configure item

################ Status ################
latent=true
init_relevance=glove
levelup_threshold=0.2
leveldown_threshold=0.05
times=3,5
mode=train
# string: train/interactive_predict

################ Datasets(Input/Output) ################
datasets_fold=../data
train_file=cnews.train.txt
# toutiao_cat_data
dev_file=cnews.val.txt

delimiter=`
# string: (t: "\t";"table")|(b: "backspace";" ")|(other, e.g., '|||', ...)

vocabs_dir=../data/vocabs

log_dir=../data/logs

tmp_dir=../data/tmp

checkpoints_dir=../checkpoints

model_save_dir=saved_model/

measuring_metrics=[accuracy]
# string: accuracy|precision|recall|f1
# f1 is compulsory

################ Model Configuration ################
embedding_dim=300
# int, must be consistent with `token_emb_dir' file

hidden_size=768

max_sequence_length=512
# int, cautions! set as a LARGE number as possible,
# this will be kept during training and inferring, text having length larger than this will be truncated.

CUDA_VISIBLE_DEVICES=0
# coincides with tf.CUDA_VISIBLE_DEVICES

seed=42

################ Training Settings ###
version=0
pretrained_model_name=bert-base-chinese
epoch=6
batch_size=6

dropout=0.10
learning_rate=0.0001

optimizer=Adam
# string: SGD/Adagrad/AdaDelta/RMSprop/Adam

checkpoints_max_to_keep=3
print_per_batch=150

is_early_stop=True
patient=2
# unnecessary if is_early_stop=False

checkpoint_name=model
